{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbafa8fc",
   "metadata": {},
   "source": [
    "# Convert Excel as a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read xlsx file\n",
    "df = pd.read_excel('rickandmorty.xlsx')\n",
    "\n",
    "# Open a new txt file for writing\n",
    "with open('conversations.txt', 'w') as f:\n",
    "    for i in range(0, len(df), 4):\n",
    "        chunk = df[i:i + 4]\n",
    "        conversation = ''\n",
    "        for index, row in chunk.iterrows():\n",
    "            \n",
    "            #conversation += f\"<|{row['isim']}|> {row['astar']} - \"\n",
    "            if index%2==0:\n",
    "                conversation += f\"<|Rick|> {row['astar']} \"\n",
    "            else:\n",
    "                conversation += f\"<|Morty|> {row['astar']} \"\n",
    "        f.write(f\"{conversation[:]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4485d9",
   "metadata": {},
   "source": [
    "# Split the Text File as Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(file_path, train_file, test_file, test_size=0.15):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    num_test_lines = int(len(lines) * test_size)\n",
    "    with open(train_file, 'w', encoding=\"utf-8\") as f:\n",
    "        f.writelines(lines[num_test_lines:])\n",
    "    with open(test_file, 'w' , encoding=\"utf-8\") as f:\n",
    "        f.writelines(lines[:num_test_lines])\n",
    "\n",
    "split_file(\"conversations.txt\", \"train_dataset.txt\", \"test_dataset.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83412033",
   "metadata": {},
   "source": [
    "# Get model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130f6c3",
   "metadata": {},
   "source": [
    "# Set the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=32)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=32)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-turkish\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=24, # number of training epochs\n",
    "    per_device_train_batch_size=24, # batch size for training\n",
    "    per_device_eval_batch_size=48,  # batch size for evaluation\n",
    "    save_steps=256, # after # steps model is saved \n",
    "    warmup_steps=256,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=6,\n",
    "    max_steps = 36,\n",
    "    evaluation_strategy=\"steps\"\n",
    "    #learning_rate = 1e3\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb8eaa",
   "metadata": {},
   "source": [
    "# Get number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "tokenizer_conversation = tokenizer.encode(content)\n",
    "print(\"Number of Tokens:\", len(tokenizer_conversation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc7320",
   "metadata": {},
   "source": [
    "# Train and save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4f4f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e02f3",
   "metadata": {},
   "source": [
    "# Configuration of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ae950",
   "metadata": {},
   "source": [
    "# Setup the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a310f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "chef = pipeline('text-generation',model='./gpt2-turkish', tokenizer=\"redrussianarmy/gpt2-turkish-cased\", pad_token_id=50256, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def find_third_lt(s):\n",
    "    count = 0\n",
    "    for i, c in enumerate(s):\n",
    "        if c == \"<\":\n",
    "            count += 1\n",
    "            if count == 3:\n",
    "                return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    morty = input()\n",
    "    rick = chef(f\"Bu, gelişmiş bir yapay zeka olan Rick ve insan Morty arasındaki bir konuşmadır: <|Morty|> {morty}\\n\")[0][\"generated_text\"]\n",
    "    third_small = find_third_lt(rick)\n",
    "    #print(rick)\n",
    "    #print(\"-----------------------------------------\")\n",
    "    print(\"Rick: \" + rick[100 + len(morty):third_small])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7707670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
